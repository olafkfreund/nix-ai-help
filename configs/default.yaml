default:
    ai_provider: ollama  # Options: openai, ollama, gemini, llamacpp, custom, claude
    ai_model: llama3  # Model name for ollama or gemini, e.g., "llama3", "gemini-1.5-flash"
    # For llamacpp, set endpoint via LLAMACPP_ENDPOINT env var (default: http://localhost:8080/completion)
    # Example: export LLAMACPP_ENDPOINT="http://localhost:8080/completion"
    # Example configuration for llamacpp provider:
    # ai_provider: llamacpp
    # ai_model: llama-2-7b-chat
    # Set endpoint via environment variable:
    #   export LLAMACPP_ENDPOINT="http://localhost:8080/completion"
    # If unset, defaults to http://localhost:8080/completion
    # Custom AI provider configuration (used if ai_provider: custom)
    custom_ai:
        base_url: http://localhost:8080/api/generate  # HTTP API endpoint URL
        headers:  # Optional custom headers (e.g., for authentication)
            Authorization: "Bearer your-api-key-here"
            # Content-Type: "application/json"  # Set automatically if not provided
    # AI Models Configuration - Enhanced model management system
    ai_models:
        # Provider definitions with their configuration
        providers:
            ollama:
                name: "Ollama"
                description: "Local AI model provider for privacy-focused inference"
                type: "local"
                base_url: "http://localhost:11434"
                available: true
                supports_streaming: true
                supports_tools: true
                requires_api_key: false
                models:
                    llama3:
                        name: "Llama 3"
                        description: "Meta's Llama 3 model for general-purpose tasks"
                        size: "8B"
                        type: "chat"
                        context_window: 8192
                        max_tokens: 4096
                        recommended_for: ["nixos", "general", "coding"]
                        requires_download: true
                        default: true
                    llama3:70b:
                        name: "Llama 3 70B"
                        description: "Larger Llama 3 model for complex tasks"
                        size: "70B"
                        type: "chat"
                        context_window: 8192
                        max_tokens: 4096
                        recommended_for: ["complex", "analysis", "detailed"]
                        requires_download: true
                    codellama:
                        name: "Code Llama"
                        description: "Specialized model for code generation and analysis"
                        size: "13B"
                        type: "code"
                        context_window: 16384
                        max_tokens: 4096
                        recommended_for: ["coding", "nix", "debugging"]
                        requires_download: true
                    mistral:
                        name: "Mistral"
                        description: "Efficient model for general tasks"
                        size: "7B"
                        type: "chat"
                        context_window: 8192
                        max_tokens: 4096
                        recommended_for: ["general", "fast"]
                        requires_download: true
            gemini:
                name: "Google Gemini"
                description: "Google's advanced AI models via API"
                type: "cloud"
                base_url: "https://generativelanguage.googleapis.com/v1beta"
                available: true
                supports_streaming: true
                supports_tools: true
                requires_api_key: true
                env_var: "GEMINI_API_KEY"
                models:
                    gemini-2.5-flash-preview-05-20:
                        name: "Gemini 2.5 Flash Preview 05-20"
                        description: "Latest Gemini 2.5 Flash with adaptive thinking and cost efficiency"
                        type: "chat"
                        context_window: 1048576
                        max_tokens: 8192
                        recommended_for: ["fast", "general", "nixos", "adaptive"]
                        cost_tier: "standard"
                        default: true
                    gemini-2.5-flash-preview-native-audio-dialog:
                        name: "Gemini 2.5 Flash Native Audio"
                        description: "Gemini 2.5 Flash with native audio dialog capabilities"
                        type: "multimodal"
                        context_window: 1048576
                        max_tokens: 8192
                        recommended_for: ["audio", "multimodal", "conversational"]
                        cost_tier: "premium"
                    gemini-2.5-flash-exp-native-audio-thinking-dialog:
                        name: "Gemini 2.5 Flash Audio Thinking"
                        description: "Experimental Gemini 2.5 Flash with audio thinking capabilities"
                        type: "multimodal"
                        context_window: 1048576
                        max_tokens: 8192
                        recommended_for: ["experimental", "audio", "thinking", "conversational"]
                        cost_tier: "premium"
                    gemini-2.5-flash-preview-tts:
                        name: "Gemini 2.5 Flash TTS"
                        description: "Gemini 2.5 Flash with text-to-speech capabilities"
                        type: "tts"
                        context_window: 1048576
                        max_tokens: 8192
                        recommended_for: ["tts", "audio_generation"]
                        cost_tier: "standard"
            openai:
                name: "OpenAI"
                description: "OpenAI's GPT models via API"
                type: "cloud"
                base_url: "https://api.openai.com"
                available: true
                supports_streaming: true
                supports_tools: true
                requires_api_key: true
                env_var: "OPENAI_API_KEY"
                models:
                    gpt-4:
                        name: "GPT-4"
                        description: "Most capable GPT model for complex reasoning"
                        type: "chat"
                        context_window: 128000
                        max_tokens: 4096
                        recommended_for: ["complex", "analysis", "reasoning"]
                        cost_tier: "premium"
                        default: true
                    gpt-4-turbo:
                        name: "GPT-4 Turbo"
                        description: "Latest GPT-4 with improved performance"
                        type: "chat"
                        context_window: 128000
                        max_tokens: 4096
                        recommended_for: ["complex", "fast", "coding"]
                        cost_tier: "premium"
                    gpt-3.5-turbo:
                        name: "GPT-3.5 Turbo"
                        description: "Fast and cost-effective model"
                        type: "chat"
                        context_window: 16385
                        max_tokens: 4096
                        recommended_for: ["general", "fast", "budget"]
                        cost_tier: "standard"
            llamacpp:
                name: "Llama.cpp"
                description: "Local CPU-optimized inference server"
                type: "local"
                base_url: "http://localhost:8080"
                available: true
                supports_streaming: true
                supports_tools: false
                requires_api_key: false
                env_var: "LLAMACPP_ENDPOINT"
                models:
                    llama-2-7b-chat:
                        name: "Llama 2 7B Chat"
                        description: "CPU-optimized Llama 2 model"
                        size: "7B"
                        type: "chat"
                        context_window: 4096
                        max_tokens: 2048
                        recommended_for: ["local", "cpu", "privacy"]
                        requires_download: true
                        default: true
            custom:
                name: "Custom API"
                description: "User-defined API endpoint"
                type: "custom"
                base_url: "http://localhost:8080/api/generate"
                available: true
                supports_streaming: false
                supports_tools: false
                requires_api_key: false
                models:
                    custom-model:
                        name: "Custom Model"
                        description: "User-defined model via custom API"
                        type: "chat"
                        context_window: 4096
                        max_tokens: 2048
                        recommended_for: ["custom", "experimental"]
                        default: true
            claude:
                name: "Claude"
                description: "Anthropic's Claude models for advanced reasoning and code understanding"
                type: "cloud"
                base_url: "https://api.anthropic.com/v1/messages"
                available: true
                supports_streaming: false
                supports_tools: false
                requires_api_key: true
                models:
                    claude-3-5-sonnet-20241022:
                        name: "Claude 3.5 Sonnet"
                        description: "Most recent Claude model with excellent performance and accuracy"
                        context_size: 200000
                        capabilities: ["general", "code", "diagnosis", "reasoning", "explanation"]
                    claude-3-opus-20240229:
                        name: "Claude 3 Opus"
                        description: "Most powerful Claude model with enhanced reasoning"
                        context_size: 200000  
                        capabilities: ["general", "code", "diagnosis", "reasoning", "explanation"]
                    claude-3-sonnet-20240229:
                        name: "Claude 3 Sonnet"
                        description: "Balanced Claude model for most tasks"
                        context_size: 180000
                        capabilities: ["general", "code", "diagnosis", "reasoning"]
                    claude-3-haiku-20240307:
                        name: "Claude 3 Haiku"
                        description: "Fastest Claude model for routine tasks"
                        context_size: 150000
                        capabilities: ["general", "code", "diagnosis"]
        # Model selection preferences
        selection_preferences:
            # Default provider when none specified
            default_provider: "ollama"
            # Default model per provider (overrides model defaults above)
            default_models:
                ollama: "llama3"
                gemini: "gemini-2.5-flash-preview-05-20"
                openai: "gpt-3.5-turbo"
                llamacpp: "llama-2-7b-chat"
                custom: "custom-model"
                claude: "claude-3-5-sonnet-20241022"
            # Task-specific model recommendations
            task_models:
                nixos_config:
                    primary: ["claude:claude-3-5-sonnet-20241022", "ollama:llama3", "gemini:gemini-2.5-flash-preview-05-20"]
                    fallback: ["openai:gpt-3.5-turbo"]
                code_generation:
                    primary: ["claude:claude-3-5-sonnet-20241022", "ollama:codellama", "openai:gpt-4"]
                    fallback: ["gemini:gemini-2.5-flash-preview-05-20"]
                debugging:
                    primary: ["claude:claude-3-opus-20240229", "openai:gpt-4", "gemini:gemini-2.5-flash-preview-05-20"]
                    fallback: ["ollama:llama3"]
                general_help:
                    primary: ["claude:claude-3-sonnet-20240229", "ollama:llama3", "gemini:gemini-2.5-flash-preview-05-20"]
                    fallback: ["openai:gpt-3.5-turbo"]
                complex_analysis:
                    primary: ["claude:claude-3-opus-20240229", "openai:gpt-4", "gemini:gemini-2.5-flash-preview-05-20"]
                    fallback: ["ollama:llama3:70b"]
        # Model discovery and availability checking
        discovery:
            # Whether to auto-discover available models on startup
            auto_discover: true
            # Cache discovery results for this duration (seconds)
            cache_duration: 3600
            # Timeout for availability checks (seconds)
            check_timeout: 10
            # Retry count for failed checks
            max_retries: 2
    log_level: debug
    mcp_server:
        host: localhost
        port: 8081
        socket_path: /tmp/nixai-mcp.sock
        auto_start: false
        documentation_sources:
            - nixos-options-es://options  # Special endpoint for NixOS options via ElasticSearch
            - https://wiki.nixos.org/wiki/NixOS_Wiki
            - https://nix.dev/manual/nix
            - https://nix.dev/  # Added main nix.dev site as a documentation source
            - https://nixos.org/manual/nixpkgs/stable/
            - https://nix.dev/manual/nix/2.28/language/
            - https://nix-community.github.io/home-manager/
    nixos:
        config_path: /etc/nixos/configuration.nix
        log_path: /var/log/nixos.log
    diagnostics:
        enabled: true
        threshold: 5
        error_patterns:
            # Example user-defined error patterns
            - name: custom_permission_error
              pattern: '(?i)custom permission denied|access refused by policy'
              error_type: permission
              severity: medium
              description: Custom permission or access error
            - name: custom_network_issue
              pattern: '(?i)custom network unreachable|host not found'
              error_type: network
              severity: high
              description: Custom network connectivity issue
    commands:
        timeout: 30
        retries: 3
    # AI provider-specific timeout configurations (in seconds)
    ai_timeouts:
        ollama: 60          # Local Ollama server timeout
        llamacpp: 120       # LlamaCpp server timeout (often slower due to CPU inference)
        gemini: 30          # Google Gemini API timeout
        openai: 30          # OpenAI API timeout
        claude: 60          # Anthropic Claude API timeout
        custom: 60          # Custom API timeout
        default: 60         # Default timeout for unspecified providers
    devenv:
        default_directory: "."
        auto_init_git: true
        templates:
            python:
                enabled: true
                default_version: "311"
                default_package_manager: "pip"
            rust:
                enabled: true
                default_version: "stable"
            nodejs:
                enabled: true
                default_version: "20"
                default_package_manager: "npm"
            golang:
                enabled: true
                default_version: "1.21"
